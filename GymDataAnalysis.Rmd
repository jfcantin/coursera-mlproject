---
title: "Unilateral Dumbbell Biceps Curl Motion Analysis"
author: "Jean-Francois Cantin"
date: "April 22, 2015"
output:
  html_document:
    fig_height: 7
    fig_width: 10
    toc: yes
---

```{r initialization, include=FALSE}
rm(list=ls())
library(stringr)
library(tidyr)
library(plyr)
library(dplyr)
library(httr)
library(ggplot2)
library(lubridate)
library(caret)
library(randomForest)
```

```{r setupCluster, include=FALSE}
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

# Summary
Using the weight lifting exercise dataset from the [human activity recognition website](http://groupware.les.inf.puc-rio.br/har) we tried to predict the manner in which the subjects performed Dumbbell Biceps Curl. The training data contained motion analysis from 6 participants each performing 10 repetitions in five different fashions: 

* exactly according to the specification (Class A) 
* throwing the elbows to the front (Class B)
* lifting the dumbbell only halfway (Class C)
* lowering the dumbbell only halfway (Class D) 
* throwing the hips to the front (Class E)

Multiple classification models (CART, Boosting, Random Forest) were tried and the random forest model with 100 trees was selected for its accuracy and its computational speed.

# Get Data
```{r originalData}
# The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har
trainingFile <- "data//training.rds"
validationFile <- "data//validation.rds"
if(file.exists(trainingFile) && file.exists(validationFile)){
  otraining <- readRDS(trainingFile)
  ovalidation <- readRDS(validationFile)
} else{
  pmlTrainingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  pmlValidationUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  otraining <- tbl_df(content(GET(pmlTrainingUrl)))
  ovalidation <- tbl_df(content(GET(pmlValidationUrl)))
  saveRDS(otraining, trainingFile)
  saveRDS(ovalidation, validationFile)
}
```

# Data Exploration

## Data Cleaning
```{r initialDataReview, warning=FALSE}
dim(otraining)
glimpse(otraining[1:20])
```
We can see that there are a few columns with empty strings. It seems that the empty strings (`character`) columns are just misclassified and should be reclassified as double. Same goes for the timestamps columns that are labeled as `character`.

```{r columnClassification, warning=FALSE}
# reorder columns
training <- otraining %>% select(classe, user_name:magnet_forearm_z)

# create factors
factorCols <- c(1,2,6)
training[factorCols] <- lapply(training[factorCols], as.factor)

# create dates
training$cvtd_timestamp <- dmy_hm(training$cvtd_timestamp)

# convert char to double
ischarCols <- sapply(training, is.character)
training[ischarCols] <- lapply(training[ischarCols], as.double)

# combine timestamps
training <- training %>% 
  mutate(rawtimestamp = as.double(paste(raw_timestamp_part_1, raw_timestamp_part_2, sep="."))) %>% 
  select(rawtimestamp, classe, user_name, roll_belt:magnet_forearm_z)

```

There appears to be lots of NA's so we need a strategy in dealing with them.

After reading the [paper](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) associated with the data we can conclude that most of the variables with significant numbers of NA's are calculated based on the paper author's window strategy. 
With that in mind the only raw data we should use are the original time series since any derived / calculated features will not be present in the test dataset. 
So we will be excluding the following columns (kurtosis, skewness, max, min, amplitude, var, avg, stddev)

```{r columnSelection}
training <- training %>% 
  select(-starts_with("kurtosis_"),
         -starts_with("skewness_"),
         -starts_with("max_"),
         -starts_with("min_"),
         -starts_with("amplitude_"),
         -starts_with("var_"),
         -starts_with("avg_"),
         -starts_with("stddev_"))
```

Now that we have removed the unnecessary columns we can have a quick look at some of the data.

```{r explore}
p <- ggplot(training)
p <- p + geom_line(aes(x=rawtimestamp, y=total_accel_dumbbell)) + aes(colour=classe)
p <- p + facet_grid(classe ~ user_name, scales = "free_x")
p

p <- ggplot(training)
p <- p + geom_line(aes(x=rawtimestamp, y=total_accel_belt)) + aes(colour=classe)
p <- p + facet_grid(classe ~ user_name, scales = "free_x")
p

```

# Model Creation
## Data Splitting
```{r samplesets}
set.seed(123)
inTrain <- createDataPartition(training$classe, p = .75,)[[1]]
# remove timestamp, classes and user_name from predictors
trainsetOutcome <- training[inTrain,2]
trainsetPredictor <- training[inTrain,-(1:3)]
testsetOutcome <- training[-inTrain,2]
testsetPredictor <- training[-inTrain,-(1:3)]
```

## Feature selection
### Near Zero Variance
```{r nzv}
# Find near zero variance features
nzv <- nearZeroVar(trainsetPredictor, saveMetrics = TRUE)
nzv %>% filter(nzv==TRUE)
```
There were no features with near zero variance.

### Identify Correlated Predictors
```{r correlatedPredictors}
correlationMatrix <- cor(trainsetPredictor)
heatmap(correlationMatrix)
highCorrelationFeatureIdx <- findCorrelation(correlationMatrix, cutoff = 0.75)

# remove redundant features
trainSubset <- trainsetPredictor[,-highCorrelationFeatureIdx]
testSubset <- testsetPredictor[,-highCorrelationFeatureIdx]
dim(trainSubset)
```
After removing the highly correlated predictors we are down to 31 features. 
We can now start the processing of these variables.

## Model and Predict
### Scale and center data
```{r scaleCenter}
set.seed(1)
centerScaledTrainSubset <- preProcess(trainSubset, method=c("center", "scale"))
trainSubsetTransformed <- predict(centerScaledTrainSubset, trainSubset)
testSubsetTransformed <- predict(centerScaledTrainSubset, testSubset)
```

### Model with Rpart
```{r rpart, cache=TRUE, message=FALSE}
set.seed(1)
control <- trainControl(method="cv", number=10)
modelRpart <- train(trainsetOutcome$classe ~ ., data=trainSubsetTransformed, method="rpart", trControl=control)
importanceRpart <- varImp(modelRpart, scale=FALSE)
```

### Model with boosting
```{r boosting, cache=TRUE, message=FALSE}
# boosting
set.seed(1)
modelBoosting <- train(trainsetOutcome$classe ~ ., data=trainSubsetTransformed, 
                       method="gbm", 
                       trControl=control, 
                       verbose = FALSE)
importanceBoosting <- varImp(modelBoosting, scale=FALSE)
```

### Model with Random Forest
```{r randomForest50, cache=TRUE, message=FALSE}
set.seed(1)
modelRF50 <- train(trainsetOutcome$classe ~ .,
                data=trainSubsetTransformed,
                method="rf",
                trControl=control,
                ntree=50)
importanceRF50 <- varImp(modelRF50, scale=FALSE)
```

```{r randomForest100, cache=TRUE, message=FALSE}
set.seed(1)
modelRF100 <- train(trainsetOutcome$classe ~ .,
                data=trainSubsetTransformed,
                method="rf",
                trControl=control,
                ntree=100)
importanceRF100 <- varImp(modelRF100, scale=FALSE)
```

```{r randomForest250, cache=TRUE, message=FALSE}
set.seed(1)
modelRF250 <- train(trainsetOutcome$classe ~ .,
                data=trainSubsetTransformed,
                method="rf",
                trControl=control,
                ntree=250)
importanceRF250 <- varImp(modelRF250, scale=FALSE)
```

## Compare models
```{r compareModels}
plot(modelRF250$finalModel)

results <- resamples(
  list( 
    RPART=modelRpart,  
    GBM=modelBoosting,
    RF_50=modelRF50,
    RF_100=modelRF100,
    RF_250=modelRF250
    ))
summary(results)

results <- resamples( list(  
  GBM=modelBoosting, RF_50=modelRF50,RF_100=modelRF100,
    RF_250=modelRF250
    ))
bwplot(results)
```

We can see from the summary table the the CART model (using rpart) has a very low accuracy and was therefore dropped from the Accuracy / Kappa plot.

All models were run with a 10 fold cross validation.

On the box plot of the Accuracy and Kappa the random forest models are far more accurate than the boosting model. The three random forest models have very close accuracy to each other although both models with 100 and 250 trees have slightly better metrics. Given that the random forest model with 100 trees was significantly faster to compute and his in sample error is very close to the model with 250 trees, therefore the random forest model with 100 trees was selected. 

## In Sample Error Rate
```{r}
modelRF100$finalModel
```
The in sample estimate of error rate calculated for the final model of the random forest model with 100 trees is 0.92% which translate into a 99.08% accuracy for the classification.

## Apply model to test data
```{r testdata}
set.seed(1)
testConfusionMatrix <- confusionMatrix(testsetOutcome$classe, predict(modelRF100, testSubsetTransformed))
testConfusionMatrix$overall
```

## Out of Sample Error Rate
Using the test set we calculated an accuracy of 99.08% which translate into an out of sample error rate of 0.92%. So it seems that the model was not over fitting the data since its out of sample error rate is similar to the in sample error rate.

# References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

```{r validateModel, eval=FALSE, include=FALSE}
# Prepare TestFile

colnames(trainset)
validationSet <- otesting[colnames(trainSubsetTransformed)]
validationPrediction <- predict(modelRF50, validationSet)
pml_write_files(as.character(validationPrediction))
```


```{r LastWords, include=FALSE}
stopCluster(cl)
sessionInfo()
```

